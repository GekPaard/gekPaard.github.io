{"name":"Gekpaard.GitHub.io","tagline":"courseraML","body":"---\r\ntitle: \"Machine Learning Exercise\"\r\nauthor: \"Gek Paard\"\r\ndate: \"Friday, March 13, 2015\"\r\noutput: html_document\r\n---\r\n\r\n## Introduction\r\nIn the MOOC course \"Practical Machine Learning\" of the Johns Hopkins University the project is to investigate the Weight Lifting Exercises Dataset which is generously provided by the investigators. See their paper: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) Stuttgart, Germany: ACM SIGCHI, 2013. See also http://groupware.les.inf.puc-rio.br/har.  \r\nThe dataset consists of measurements by sensors on six individuals who performed the Unilateral Dumbbell Biceps Curl. The measurements were made by sensors on the forearm, biceps, belt and dumbbell. The curl had to be performed in five different ways. This manner is found in the variable \"classe\" in the training set. Classes, class \"A\" is the good one, and the classes \"B\", \"C\", \"D\" and \"E\" are wrong performances. The subjects were superviced by trained personell who made sure that the exercises where performed in the correct right or wrong manner.\r\nThe purpose of this exercise is to predict from the measurements the manner in which they did the exercises.\r\n\r\n##Investigation\r\nFirst of all I loaded the required packages needed for this investigation. \r\n> \r\nlibrary(caret)\r\nlibrary(randomForest)\r\n> \r\nThen I imported the training set, which was already downloaded in the workspace\r\n```{r}\r\ntrainset <- read.csv(\"pml-training.csv\")\r\ndim(trainset)\r\n```\r\nAs can be seen there a lot of variables. Many of them wil not be relevant predictors. The first 7 variables shouldn't be relevant, because they are about who performed the exercise, on which time, the following number etc. So I removed them from the dataset. Then I set the abundance of NA's to zero's, because randomForest can't cope with NA's. (I choose randomForest because the train function of caret takes a very long time.) Then I removed all the variables with little or no variation from the dataset. They will not be important predictors. \r\n```{r}\r\ntraindata   <- trainset\r\ntraindata   <- traindata[,-(1:7)]\r\ntraindata[is.na(traindata)] <- 0\r\nnzv         <- nearZeroVar(traindata)\r\ntraindata   <- traindata[-nzv]\r\n```\r\nThen I checked for correlations in the remaining variables. There are several.  \r\n```{r}\r\nM <- abs(cor(traindata[,-53]))\r\ndiag(M) <- 0\r\ncorrelations <- which(M> 0.8, arr.ind=T)\r\ncorrelations\r\n\r\n```\r\nIf a variable correlates with only one other variable I choose to remove the one with the least variation. See the plots below. If variables correlate with more than one other variables I choose to keep the one which has the most correlations. For instance the variable roll-belt correlates with yaw-belt, total-accel-belt, accel-belt-y and accel-belt-z. But yaw-belt only correlates to roll-belt. roll-belt, total-accel-belt, accel-belt-y and accel-belt-ze correlates to each other.     \r\n```{r}\r\nsmallTrain <- traindata[,c(18,19)]\r\nprComp <- prcomp(smallTrain)\r\nplot(prComp$x[,1], prComp$x[,2], xlab= \"gyros_arm_x (18)\", ylab = \"gyros_arm_x (19)\")\r\nsmallTrain <- traindata[,c(21,24)]\r\nprComp <- prcomp(smallTrain)\r\nplot(prComp$x[,1], prComp$x[,2], xlab= \"accel_arm_x (21)\", ylab = \"magnet_arm_x (24)\")\r\nsmallTrain <- traindata[,c(25,26)]\r\nprComp <- prcomp(smallTrain)\r\nplot(prComp$x[,1], prComp$x[,2], xlab= \"magnet_arm_y (25)\", ylab = \"magnet_arm_Z (26)\")\r\nsmallTrain <- traindata[,c(28,34)]\r\nprComp <- prcomp(smallTrain)\r\nplot(prComp$x[,1], prComp$x[,2], xlab= \"pitch_dumbbell (28)\", ylab = \"accel_dumbbell_x (34)\")\r\nsmallTrain <- traindata[,c(29,36)]\r\nprComp <- prcomp(smallTrain)\r\nplot(prComp$x[,1], prComp$x[,2], xlab= \"yaw_dumbbell (29)\", ylab = \"accel_dumbbell_z (36)\")\r\n```\r\n```{r}\r\ntraindata <- traindata[,-45]\r\ntraindata <- traindata[,-36]\r\ntraindata <- traindata[,-34]\r\ntraindata <- traindata[,-33]\r\ntraindata <- traindata[,-31]\r\ntraindata <- traindata[,-26]\r\ntraindata <- traindata[,-24]\r\ntraindata <- traindata[,-19]\r\ntraindata <- traindata[,-11]\r\ntraindata <- traindata[,-10]\r\ntraindata <- traindata[,-9]\r\ntraindata <- traindata[,-8]\r\ntraindata <- traindata[,-4]\r\ntraindata <- traindata[,-3]\r\ndim(traindata)\r\n```\r\nThere are now 38 potential predictors available, which is fortunate because the function randomForest can only handle 53 predictors.  \r\nA check if the remaining variables are good predictors:  \r\n```{r}\r\nmodFit1 <- randomForest(classe ~ ., data=traindata)\r\npredictionClassedata  <- predict(modFit1, trainset)\r\nconfusionMatrix(predictionClassedata, trainset$classe)\r\nvarImp(modFit1)\r\n\r\n```\r\n  \r\nThis seems like a perfect fit!  \r\nThe function varImpPlot shows the importance of the predictors. I want to check if als the variables are needed for a good prediction. This time I will use only the variables with an importance of 500 and higher:\r\n```{r}\r\nmodFit2 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt           +\r\n                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x + magnet_belt_y +\r\n                                 magnet_belt_z + accel_dumbbell_y + roll_dumbbell, data=traindata)\r\npredictionClassedata  <- predict(modFit2, trainset)\r\nconfusionMatrix(predictionClassedata, trainset$classe)\r\nvarImp(modFit2)\r\n\r\n```\r\n  \r\nThis still looks like a perfect fit. Now I remove the lowest important predictor, magnet-belt-y, from the predictors.  \r\n```{r}\r\nmodFit3 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt  +\r\n                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x        + \r\n                                 magnet_belt_z + accel_dumbbell_y + roll_dumbbell, data=traindata)\r\npredictionClassedata  <- predict(modFit3, trainset)\r\nconfusionMatrix(predictionClassedata, trainset$classe)\r\nvarImp(modFit3)\r\n\r\n```\r\n  \r\nThis also looks like a perfect fit. Now I remove again the lowest important predictor of this set, roll-dumbbell, from the predictors.  \r\n```{r}\r\nmodFit4 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +\r\n                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x       + \r\n                                 magnet_belt_z + accel_dumbbell_y , data=traindata)\r\npredictionClassedata  <- predict(modFit4, trainset)\r\nconfusionMatrix(predictionClassedata, trainset$classe)\r\nvarImp(modFit4)\r\n\r\n```\r\n  \r\nThis also looks like a perfect fit. Again I remove the lowest important predictor of this set, magnet-dumbbell-x, from the predictors.\r\n```{r}\r\nmodFit5 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +\r\n                          magnet_dumbbell_y + roll_forearm + magnet_belt_z + accel_dumbbell_y , data=traindata)\r\npredictionClassedata  <- predict(modFit5, trainset)\r\nconfusionMatrix(predictionClassedata, trainset$classe)\r\n```\r\n  \r\nThis isn't a perfect fit anymore. So I use modFit4 for the prediction of the classes of testdata, which I also placed in the workspace. \r\n```{r}\r\ntestdata <- read.csv (\"pml-testing.csv\")\r\npredict(modFit4, testdata)\r\n```\r\n\r\n#Conclusion\r\nI selected nine variables out of 159 possible ones, and reached an accuracy of 100%. So the expected out-of-sample error should be near 0%. > ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}