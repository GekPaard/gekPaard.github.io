---
title: "Machine Learning Exercise"
author: "Gek Paard"
date: "Friday, March 13, 2015"
output: html_document
---

## Introduction
In the MOOC course "Practical Machine Learning" of the Johns Hopkins University the project is to investigate the Weight Lifting Exercises Dataset which is generously provided by the investigators. See their paper: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) Stuttgart, Germany: ACM SIGCHI, 2013. See also http://groupware.les.inf.puc-rio.br/har.  
The dataset consists of measurements by sensors on six individuals who performed the Unilateral Dumbbell Biceps Curl. The measurements were made by sensors on the forearm, biceps, belt and dumbbell. The curl had to be performed in five different ways. This manner is found in the variable "classe" in the training set. Classes, class "A" is the good one, and the classes "B", "C", "D" and "E" are wrong performances. The subjects were superviced by trained personell who made sure that the exercises where performed in the correct right or wrong manner.
The purpose of this exercise is to predict from the measurements the manner in which they did the exercises.

##Investigation
First of all I loaded the required packages needed for this investigation. 
```{r, results='hide'}
library(caret)
library(randomForest)
```
Then I imported the training set, which was already downloaded in the workspace
```{r}
trainset <- read.csv("pml-training.csv")
dim(trainset)
```
As can be seen there a lot of variables. Many of them wil not be relevant predictors. The first 7 variables shouldn't be relevant, because they are about who performed the exercise, on which time, the following number etc. So I removed them from the dataset. Then I set the abundance of NA's to zero's, because randomForest can't cope with NA's. (I choose randomForest because the train function of caret takes a very long time.) Then I removed all the variables with little or no variation from the dataset. They will not be important predictors. 
```{r}
traindata   <- trainset
traindata   <- traindata[,-(1:7)]
traindata[is.na(traindata)] <- 0
nzv         <- nearZeroVar(traindata)
traindata   <- traindata[-nzv]
```
Then I checked for correlations in the remaining variables. There are several.  
```{r}
M <- abs(cor(traindata[,-53]))
diag(M) <- 0
correlations <- which(M> 0.8, arr.ind=T)
correlations

```
If a variable correlates with only one other variable I choose to remove the one with the least variation. See the plots below. If variables correlate with more than one other variables I choose to keep the one which has the most correlations. For instance the variable roll-belt correlates with yaw-belt, total-accel-belt, accel-belt-y and accel-belt-z. But yaw-belt only correlates to roll-belt. roll-belt, total-accel-belt, accel-belt-y and accel-belt-ze correlates to each other.     
```{r}
smallTrain <- traindata[,c(18,19)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "gyros_arm_x (18)", ylab = "gyros_arm_x (19)")
smallTrain <- traindata[,c(21,24)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "accel_arm_x (21)", ylab = "magnet_arm_x (24)")
smallTrain <- traindata[,c(25,26)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "magnet_arm_y (25)", ylab = "magnet_arm_Z (26)")
smallTrain <- traindata[,c(28,34)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "pitch_dumbbell (28)", ylab = "accel_dumbbell_x (34)")
smallTrain <- traindata[,c(29,36)]
prComp <- prcomp(smallTrain)
plot(prComp$x[,1], prComp$x[,2], xlab= "yaw_dumbbell (29)", ylab = "accel_dumbbell_z (36)")
```
```{r}
traindata <- traindata[,-45]
traindata <- traindata[,-36]
traindata <- traindata[,-34]
traindata <- traindata[,-33]
traindata <- traindata[,-31]
traindata <- traindata[,-26]
traindata <- traindata[,-24]
traindata <- traindata[,-19]
traindata <- traindata[,-11]
traindata <- traindata[,-10]
traindata <- traindata[,-9]
traindata <- traindata[,-8]
traindata <- traindata[,-4]
traindata <- traindata[,-3]
dim(traindata)
```
There are now 38 potential predictors available, which is fortunate because the function randomForest can only handle 53 predictors.  
A check if the remaining variables are good predictors:  
```{r}
modFit1 <- randomForest(classe ~ ., data=traindata)
predictionClassedata  <- predict(modFit1, trainset)
confusionMatrix(predictionClassedata, trainset$classe)
varImp(modFit1)

```
  
This seems like a perfect fit!  
The function varImpPlot shows the importance of the predictors. I want to check if als the variables are needed for a good prediction. This time I will use only the variables with an importance of 500 and higher:
```{r}
modFit2 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt           +
                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x + magnet_belt_y +
                                 magnet_belt_z + accel_dumbbell_y + roll_dumbbell, data=traindata)
predictionClassedata  <- predict(modFit2, trainset)
confusionMatrix(predictionClassedata, trainset$classe)
varImp(modFit2)

```
  
This still looks like a perfect fit. Now I remove the lowest important predictor, magnet-belt-y, from the predictors.  
```{r}
modFit3 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt  +
                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x        + 
                                 magnet_belt_z + accel_dumbbell_y + roll_dumbbell, data=traindata)
predictionClassedata  <- predict(modFit3, trainset)
confusionMatrix(predictionClassedata, trainset$classe)
varImp(modFit3)

```
  
This also looks like a perfect fit. Now I remove again the lowest important predictor of this set, roll-dumbbell, from the predictors.  
```{r}
modFit4 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +
                                 magnet_dumbbell_y + roll_forearm + magnet_dumbbell_x       + 
                                 magnet_belt_z + accel_dumbbell_y , data=traindata)
predictionClassedata  <- predict(modFit4, trainset)
confusionMatrix(predictionClassedata, trainset$classe)
varImp(modFit4)

```
  
This also looks like a perfect fit. Again I remove the lowest important predictor of this set, magnet-dumbbell-x, from the predictors.
```{r}
modFit5 <- randomForest(classe ~ roll_belt + magnet_dumbbell_z + pitch_forearm + pitch_belt +
                          magnet_dumbbell_y + roll_forearm + magnet_belt_z + accel_dumbbell_y , data=traindata)
predictionClassedata  <- predict(modFit5, trainset)
confusionMatrix(predictionClassedata, trainset$classe)
```
  
This isn't a perfect fit anymore. So I use modFit4 for the prediction of the classes of testdata, which I also placed in the workspace. 
```{r}
testdata <- read.csv ("pml-testing.csv")
predict(modFit4, testdata)
```

#Conclusion
I selected nine variables out of 159 possible ones, and reached an accuracy of 100%. So the expected out-of-sample error should be near 0%. 